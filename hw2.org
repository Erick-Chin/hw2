#+title: Homework 2: Formal Languages, Parsing, and Semantics
#+author: Toni Kazic
#+date: Fall, 2024


# revised <2021-09-25 Sat>

#+SETUPFILE: "../../../common/preamble.org"
#+LATEX_CLASS: article
#+OPTIONS: toc:nil
#+OPTIONS: ^:nil

#+LATEX_HEADER: \usepackage{langsci-avm}
# http://ftp.math.purdue.edu/mirrors/ctan.org/macros/latex/contrib/langsci-avm/langsci-avm.pdf
# and see also
# https://userblogs.fu-berlin.de/langsci-press/2020/04/20/writing-avms-easily-in-latex-the-new-langsci-avm-package/


#+LATEX_HEADER: \newcommand{\grmr}[2]{\ensuremath{\mathrm{#1} & \,\longrightarrow\, \mathrm{#2}}}
#+LATEX_HEADER: \newcommand{\txtgrmr}[2]{\ensuremath{\mathrm{#1} \,\longrightarrow\, \mathrm{#2}}}
#+LATEX_HEADER: \newcommand{\grmrhs}[1]{\ensuremath{& \,\longrightarrow\, \mathrm{#1} }}
#+LATEX_HEADER: \newcommand{\wa}[1]{\type{\textnormal{\w{#1}}}}

# compile with pdflatex
#
# Kazic, 3.11.2020



* Introduction

In this homework, the syntactic and semantic rubber hits the road.  This
homework introduces the deeper structures of language, especially when
phrased formally; looks at syntax and parsing; and extends the notion of
parsing to semantics.



* Who's Who and Solution Patterns
<<whoswho>>


** Lead Person:  yellow


** Group Members

| first name last name | color                                |
|----------------------+--------------------------------------|
| Eric Chin            | yellow \color{yellow}\rule{5mm}{3mm} |
| Ying-Chen Lin        | green \color{green}\rule{5mm}{3mm}   |
| James Wiliams        | purple \color{violet}\rule{5mm}{3mm} |


** Three Member Solution Patterns

$i$ is the question number.

#+begin_center
#+ATTR_LaTeX: :mode inline-math :environment array
| \text{color}                         | \text{draft solution} | \text{revise solution} |
|--------------------------------------+-----------------------+------------------------|
| green \color{green}\rule{5mm}{3mm}   | i \mod 3 = 1          | i \mod 3 = 0           |
| yellow \color{yellow}\rule{5mm}{3mm} | i \mod 3 = 2          | i \mod 3 = 1           |
| purple \color{violet}\rule{5mm}{3mm} | i \mod 3 = 0          | i \mod 3 = 2           |
#+end_center


** Two Member Solution Patterns

| color                                | draft solution | revise solution |
|--------------------------------------+----------------+-----------------|
| green \color{green}\rule{5mm}{3mm}   | odds           | evens           |
| yellow \color{yellow}\rule{5mm}{3mm} | evens          | odds            |




* General Instructions


   + /Fill out the group members table and follow the solution patterns/ in
     Section [[whoswho]].

   + /If the question is unclear, tell me your interpretation of it as part
     of your answer./  Feel free to ask about the questions in class or on
     the Slack channel (use =@channel= as others will probably be puzzled
     too). 

   + /For questions using corpora, use the corpus of the lead person./

   + /Put your draft answers right after each question using a *complete,
     functional* =org= mode code or example block./ Make sure your code
     block is complete and functional by testing it in your copy of this
     homework file.

   + /Each group member reviews the others' draft solutions and you revise them together/.

   + /Discuss each other's draft and reviews, finalizing the answers./

   + /Show all your work: code, results, and analysis./  Does your code
     work in this file and produce *exactly* the results you show? 

   + /Post the completed file to Canvas no later than noon on the Tuesday
     indicated/ in the [[../syllabus.org::schedule][schedule in the syllabus]], naming your file with each
     person's first name (no spaces in the file name, and don't forget the
     =.org= extension!).  Only one person should submit the final file.


* Hints


** Make sure the structure of the grammar can be parsed by the parser.

For example, a recursive descent parser cannot terminate the parse of a
left-recursive grammar.



** Use re-entrancy if you need it.

=NLTK= has some notation for this.




* Questions

# added directions on proving <2023-10-20 Fri>
# clarified that the rules are from separate grammars <2023-12-11 Mon>
1. [@1] <<prod-rules>> Remember those silly tags from [[file:./hw1.org::silly-tags][hw1.org]]?  Let
#
#
\begin{align}
N &= \{ \textrm{FOO,BAR,EGO,NEED,ADS,DUCK,MANSE} \} \ \text{and} \nonumber \\
T &= \{ \w{dog, black, racing, was, squirrel, tree, burrow, ground hog, bushes, towards,
hunting, back, wee} \}  \nonumber
\end{align}
#
For each of the following production rules, state from which class of
language they come and /show why/ by derivation from the language
definitions (that is, write the proof and describe it).
   + rule 1 :: $\txtgrmr{FOO}{EGO \ NEED \ DUCK}$
     + ANSWER :: This is a production of a Context Free Language.
       + This is because the rule can be rewritten as $A \rightarrow BCD$ where $B,C,D \in N$
       + $BCD \in w$ given that $w \in \{N,T\}^*$
   + rule 2 :: $\txtgrmr{FOO \ DUCK}{EGO \ NEED \ DUCK}$
     + ANSWER :: This is a Context Sensitive Language production!
       + This is because this production is of the form $u_1 A u_2 \rightarrow u_1 w u_2$
       + $\epsilon = u_1, FOO = A, DUCK = u_2, EGO\ NEED = w$
       + The following holds true: $u_1, u_2, w \in (N \cup T)^*, A \in N, w \neq \epsilon$


   + rule 3 :: $\txtgrmr{FOO}{EGO \ \w{dog} \ DUCK}$
     + ANSWER :: This is a production of a Context Free Language.
       + This is because the rule can be rewritten as $A \rightarrow BCD$ where $B,D \in N$ and $C \in T$
       + $BCD \in w$ given that $w \in \{N,T\}^*$
   + rule 4 :: $\txtgrmr{FOO \ \w{ground hog}}{EGO \ \w{dog} \ DUCK \ \w{squirrel}}$
     + ANSWER :: This is a type âˆ… language.
       + None of the rules of regular, context-free, nor context-sensitive can represent it.
   + rule 5 :: $\txtgrmr{FOO}{\w{black} \ \w{dog} \ DUCK}$
     + ANSWER :: This is a Regular Language!
       + This is because the rule can be redescribed as $A \rightarrow XY$ where $X \in T^*$ and $Y \in N$
#
Each of the rules is abstracted from a different grammar!



2. [@2] <<which-recursn>> Consider the following grammar.
#+BEGIN_EXAMPLE
N = {A,B,C,D}
T = {foo,bar}
S = {C}
P = {C -> A B
     B -> A D
     B -> A
     D -> A A
     A -> T
     }
#+END_EXAMPLE
Is the grammar left-, right-, neither-, or both-recursive?  Why?

+ ANSWER :: The grammar is **neither left-recursive nor right-recursive**.
  - **Neither**:
    -A grammar is left-recursive if a non-terminal refers to itself on the left side.
     A grammar is right-recursive if a non-terminal refers to itself on the right side.
     None of the non-terminals in this grammar refer to themselves on the left or right side

3. [@3] <<manual-parse>> By hand, generate a construct for each unique
   length of output using the grammar of question [[which-recursn]] and show
   them and their derivation as an =org= table:

| sentence       | rule sequence and comments                                                                                                |
|----------------+---------------------------------------------------------------------------------------------------------------------------|
| $foobar$       | $S \rightarrow C \rightarrow AB \rightarrow AA \rightarrow AT \rightarrow TT \rightarrow "foo"T \rightarrow "foo"\ "bar"$ |
| $foofoofoofoo$ | $S \rightarrow C \rightarrow AB \rightarrow TB \rightarrow "foo"B \rightarrow "foo"AD \rightarrow "foo"TD \rightarrow$    |
|                | $"foo""foo"D \rightarrow "foo""foo"AA \rightarrow "foo""foo"TA \rightarrow$                                               |
|                | $"foo""foo""foo"A \rightarrow "foo""foo""foo"T \rightarrow "foo""foo""foo""foo"$                                          |


# clarified they are not to write more complex productn rules, 
# that all words are to be filtered at once, and that no trivial
# regexes are to be used.
#
# <2023-12-11 Mon>
4. [@4] <<regex>> For the terminals in question [[prod-rules]], write the
   minimum number of Python *regular expressions* (/not production rules
   from context-free or greater grammars!/) to distinguish among them /when
   the entire group of words is presented/ (not one-by-one!).  Do not use
   trivial regexes that match one and only one word.  Use conditionals and
   order the regexs into a tree until all terminals are recognized without
   ambiguities.  Carry your tree out until each terminal has a regular
   expression that places it in the leaves.  Include a sketch of your tree
   if you think it will help!

#+begin_src python :results output
import nltk

sentence = "The black dog was racing towards the tree, hunting a squirrel in a wee groundhog's burrow back in the bushes."
text = nltk.word_tokenize(sentence)
tagged_token = nltk.pos_tag(text)

grammar = r"""
  NP: {<DT>?<JJ>*(<NN.*><POS>)?<NN.*>}
  PP: {<IN><NP>}
  VP: {<VBD>?<VBG>(<RB>?<NP|PP>)*}
"""
cp = nltk.RegexpParser(grammar)
print(cp.parse(tagged_token))
#+end_src

#+RESULTS:
#+begin_example
(S
  (NP The/DT black/JJ dog/NN)
  (VP was/VBD racing/VBG (PP towards/IN (NP the/DT tree/NN)))
  ,/,
  (VP
    hunting/VBG
    (NP a/DT squirrel/NN)
    (PP in/IN (NP a/DT wee/JJ groundhog/NN 's/POS burrow/NN))
    back/RB
    (PP in/IN (NP the/DT bushes/NNS)))
  ./.)
#+end_example

5. [@5] <<regex-imp>> Now implement your regular expression tree in
   question [[regex]] and show the code and results.



6.  [@6] <<first-gram>> Write a grammar that captures the following sentences:
#
   + sentence 1 :: ``The cheerful black dog slept quietly by the chair.''
   + sentence 2 :: ``A sleepy yellow dog stretched his back.''
   + sentence 3 :: ``Somebody downstairs made the coffee.''
   +  ANSWER :: Productions Below
     + $S \rightarrow NP\ VP$
     + $NP \rightarrow Det\ ADJP\ Noun$
     + $NP \rightarrow Det\ Noun$
     + $NP \rightarrow Noun\ Adj$
     + $ADJP \rightarrow Adj\ ADJP \|\ Adj$
     + $VP \rightarrow Verb\ Adv\ PP$
     + $VP \rightarrow Verb\ NP$
     + $PP \rightarrow Prep\ NP$
     + $Det \rightarrow "the" \| "a" \| "his"$
     + $Noun \rightarrow "dog" \| "chair" \| "back" \| "somebody" \| "coffee"$
     + $Adj \rightarrow "cheerful" \| "black" \| "sleepy" \| "yellow" \| "downstairs"$
     + $Verb \rightarrow "slept" \| "stretched" \| "made"$
     + $Prep \rightarrow "by"$
     + $Adv \rightarrow "quietly"$
#
Put the phrases generated by each rule from these sentences alongside the
rules, again as an =org= table (this example is *JUST to illustrate format,
it is not correct!*):
#
| rule                           | phrase                                              |
|--------------------------------+-----------------------------------------------------|
| S $\rightarrow$ NP VP          | (The cheerful black dog slept quietly by the chair) |
| S $\rightarrow$ NP VP          | (A sleepy yellow dog stretched his back)            |
| S $\rightarrow$ NP VP          | (Somebody downstairs made the coffee)               |
| NP $\rightarrow$ Det ADJP Noun | (The cheerful black dog)                            |
| NP $\rightarrow$ Det ADJP Noun | (A sleepy yellow dog)                               |
| NP $\rightarrow$ Det Noun      | (the coffee)                                        |
| NP $\rightarrow$ Det Noun      | (his back)                                          |
| NP $\rightarrow$ Noun Adj      | (Somebody downstairs)                               |
| ADJP $\rightarrow$ Adj ADJP    | (sleepy yellow)                                     |
| ADJP $\rightarrow$ Adj ADJP    | (cheerful  black)                                   |
| ADJP $\rightarrow$ Adj         | (downstairs)                                        |
| ADJP $\rightarrow$ Adj         | (sleepy)                                            |
| ADJP $\rightarrow$ Adj         | (yellow)                                            |
| ADJP $\rightarrow$ Adj         | (cheerful)                                          |
| ADJP $\rightarrow$ Adj         | (black)                                             |
| VP $\rightarrow$ Verb Adv PP   | (slept quietly by the chair)                        |
| VP $\rightarrow$ Verb NP       | (stretched his back)                                |
| VP $\rightarrow$ Verb NP       | (made the coffee)                                   |
| PP $\rightarrow$ Prop NP       | (by the chair)                              |


# added pretty printing, so much easier to follow
#
# Kazic, 15.12.2023


7. [@7] <<recur-desc-parser>> Now implement the grammar of question
   [[first-gram]] as a recursive descent parser.  Parse each sentence, showing
   the results as a prettily printed tree, and compare them.  What do you
   observe?

#+begin_src python :results output
import nltk

grammar = nltk.CFG.fromstring("""
    S -> NP VP
    NP -> Det ADJP Noun | Det Noun | Noun Adj
    ADJP -> Adj ADJP | Adj
    VP -> Verb Adv PP | Verb NP
    PP -> Prep NP
    Det -> 'the' | 'a' | 'his'
    Noun -> 'dog' | 'chair' | 'back' | 'somebody' | 'coffee'
    Adj -> 'cheerful' | 'black' | 'sleepy' | 'yellow' | 'downstairs'
    Verb -> 'slept' | 'stretched' | 'made'
    Prep -> 'by'
    Adv -> 'quietly'
""")

rd_parser = nltk.RecursiveDescentParser(grammar)

sentences = [
    "The cheerful black dog slept quietly by the chair.",
    "A sleepy yellow dog stretched his back.",
    "Somebody downstairs made the coffee."
]

for sent in sentences:
    for tree in rd_parser.parse(sent.lower().replace(".", "").split()):
        tree.pretty_print()
#+end_src

#+RESULTS:
#+begin_example
                               S                             
        _______________________|____________                  
       NP                                   VP               
  _____|_________________       ____________|____             
 |           ADJP        |     |      |          PP          
 |      ______|_____     |     |      |      ____|___         
 |     |           ADJP  |     |      |     |        NP      
 |     |            |    |     |      |     |     ___|____    
Det   Adj          Adj  Noun  Verb   Adv   Prep Det      Noun
 |     |            |    |     |      |     |    |        |   
the cheerful      black dog  slept quietly  by  the     chair

                        S                         
       _________________|______________            
      NP                               |          
  ____|_________________               |           
 |         ADJP         |              VP         
 |     _____|_____      |        ______|___        
 |    |          ADJP   |       |          NP     
 |    |           |     |       |       ___|___    
Det  Adj         Adj   Noun    Verb   Det     Noun
 |    |           |     |       |      |       |   
 a  sleepy      yellow dog  stretched his     back

                         S                 
           ______________|____              
          |                   VP           
          |               ____|___          
          NP             |        NP       
    ______|______        |     ___|____     
  Noun          Adj     Verb Det      Noun 
   |             |       |    |        |    
somebody     downstairs made the     coffee

#+end_example

The more complex the sentence is, the more complex the tree is.
Like the first sentence, it has adj, adv, and prep, the tree is more complex.

8. <<chart-parser>> Following on, implement the grammar of question
   [[first-gram]] as a chart parser.  Parse each sentence, showing the results,
   and compare these chart parsing results to your results in question
   [[recur-desc-parser]].  What do you observe?

9. <<clock>> Extend your grammar for the sentences in question
   [[recur-desc-parser]]  so that it can parse sentences 4--6 below.  Time the
   implementation's performance for each sentence, doing this 1000 times
   for each sentence for better estimates, and put the results  in an =org= table.
   + sentence 4 :: ``We had a long walk to the park and Vinny played with
     three other dogs.''
   + sentence 5 :: ``It was sunny today but might not be tomorrow.''
   + sentence 6 :: ``There are 49 angels dancing on the head of this pin.''



#+begin_src python :results output raw
import nltk
import time

grammar = nltk.CFG.fromstring("""
    S -> NP VP | NP VP Con NP VP | NP VP Con VP | 'there' VP PP
    NP -> Det ADJP Noun | Det Noun | Noun Adj | Noun | Det Adj Noun PP | ADJP Noun | Adj Noun Part | Det Noun PP
    ADJP -> Adj ADJP | Adj
    VP -> Verb Adv PP | Verb NP | Verb PP | Verb Adj Adv | Mod Adv Verb Adv
    PP -> Prep NP
    Det -> 'the' | 'a' | 'his' | 'this'
    Con -> 'and' | 'but'
    Noun -> 'dog' | 'chair' | 'back' | 'somebody' | 'coffee' | 'we' | 'walk' | 'park' | 'vinny' | 'dogs' | 'it' | 'angels' | 'head' | 'pin'
    Adj -> 'cheerful' | 'black' | 'sleepy' | 'yellow' | 'downstairs' | 'three' | 'other' | 'long' | 'sunny' | '49'
    Mod -> 'might'
    Verb -> 'slept' | 'stretched' | 'made' | 'had' | 'played' | 'was' | 'be' | 'are'
    Part -> 'dancing'
    Prep -> 'by' | 'to' | 'with' | 'on' | 'of'
    Adv -> 'quietly' | 'today' | 'not' | 'tomorrow'
""")

rd_parser = nltk.RecursiveDescentParser(grammar)

sentences = [
    "The cheerful black dog slept quietly by the chair.",
    "A sleepy yellow dog stretched his back.",
    "Somebody downstairs made the coffee.",
    "We had a long walk to the park and Vinny played with three other dogs.",
    "It was sunny today but might not be tomorrow.",
    "There are 49 angels dancing on the head of this pin."
]

def from_sent_get_trees_generator(sent, index):
    better_sent = sent.lower().replace(".", "").split()
    trees_generator = rd_parser.parse(better_sent)
    return trees_generator


def get_parse_time(sent):
    runs = 100000
    start_time = time.time()
    [from_sent_get_trees_generator(sent, i) for i in range(runs)]
    end_time = time.time()
    total_time = end_time - start_time
    average_time = total_time/runs
    return f"| {sent} | {average_time} |"

thread_array = []
results = [get_parse_time(sent) for sent in sentences]
print("| Sentence | Time |")
print("|---------|------------|")
[print(result) for result in results]
#+end_src

#+RESULTS:
| Sentence                                                               |                  Time |
|------------------------------------------------------------------------+-----------------------|
| The cheerful black dog slept quietly by the chair.                     | 4.959135055541992e-06 |
| A sleepy yellow dog stretched his back.                                | 5.037796497344971e-06 |
| Somebody downstairs made the coffee.                                   | 4.305291175842285e-06 |
| We had a long walk to the park and Vinny played with three other dogs. | 5.863513946533203e-06 |
| It was sunny today but might not be tomorrow.                          | 5.145440101623535e-06 |
| There are 49 angels dancing on the head of this pin.                   | 5.278613567352295e-06 |

#+CAPTION: Table of Answers for Question 9
| Sentence                                                               |     Time |
|------------------------------------------------------------------------+----------|
| The cheerful black dog slept quietly by the chair.                     | 4.91e-06 |
| A sleepy yellow dog stretched his back.                                | 5.04e-06 |
| Somebody downstairs made the coffee.                                   | 4.32e-06 |
| We had a long walk to the park and Vinny played with three other dogs. | 5.82e-06 |
| It was sunny today but might not be tomorrow.                          |  5.1e-06 |
| There are 49 angels dancing on the head of this pin.                   | 5.24e-06 |
+ DISCLAIMER 1: :: The above table's time column rounds in a way the code when evaluate will not round. This is because how long the parsing takes is system dependent. The output from above is unrounded so that 0.0 won't potentially show in results
+ DISCLAIMER 2: :: My results for how long the evalutaion took was too variable for 1000 runs, so I decided to kick it up to 100000 even though the directions suggest 1000. If I wanted to do 1000, I would simply change the "runs" variable to 1000

10. [@10] <<avm-graph>> Consider this attribute-value matrix:
#
#+begin_export latex
\begin{center}
\avm{
[ CAT  & s \\
  HEAD & [ AGR   & \1 [ NUM & sg \\
                        PER & 3 ] \\
           SUBJ  & [ AGR \1 ] ] ] \\
}.
\end{center}
#+end_export
#
Draw the corresponding directed acyclic graph, ideally in Python.  (A hand-drawn figure is fine:
just photograph it and include the image below, as is done in [[file:../notes.org][notes.org]].)

#+begin_src python :results output
import matplotlib.pyplot as plt
import networkx as nx

G = nx.DiGraph()
G.add_node(" ")
G.add_node("CAT: s")
G.add_node("HEAD")
G.add_node("AGR: 1")
G.add_node("NUM: sg")
G.add_node("PER: 3")
G.add_node("SUBJ")

G.add_edges_from([
    (" ", "CAT: s"),
    (" ", "HEAD"),
    ("HEAD", "AGR: 1"),
    ("AGR: 1", "NUM: sg"),
    ("AGR: 1", "PER: 3"),
    ("HEAD", "SUBJ"),
    ("SUBJ", "AGR: 1")
])

pos = {
    " ": (0, 0),
    "CAT: s": (1, 1),
    "HEAD": (1, 0),
    "AGR: 1": (2, 1),
    "NUM: sg": (3, 1),
    "PER: 3": (3, 0),
    "SUBJ": (2, 0)
}

nx.draw(G, pos, with_labels=True, node_size=3000, font_size=14)
plt.show()
#+end_src

#+RESULTS:

11. [@11] <<basic-fea-struc>> Now extend your grammar from question [[clock]] to include features
    relevant to subject-verb agreement, using =nltk.FeatStruct()= from
    chapter nine, so that you can parse sentences 1--9.  Using
    =cp.parse()=, print and study the parse trees for each sentence.  Do
    you agree with them?  Why or why not?

   + sentence 7 :: ``The black dogs are playing with the elf toy.''
   + sentence 8 :: ``The yellow dog slept in my pajamas.''
   + sentence 9 :: ``We will take two long rides in the country next week.''

12. [@12] <<fopc>> [[../../reading/blk_2nd_ed.pdf][Chapter 10 of BLK]] and [[http://www.nltk.org/howto/semantics.html][the semantics howto]] march one through the basics
   of applying the FOPC and the \lambda calculus to reifying the semantics of
   context-free sentences.  One of the practical difficulties in this
   approach is ensuring that the implementation of the universe of
   discourse (they call it the /domain of discourse/, same thing) actually
   covers the intended universe.

   To see this, let's use their =sem2.fcfg= grammar to parse the following
   sentences syntactically and semantically, and output the reification of
   the sentences into the FOPC and the \lambda calculus.

   For each of the following sentences, parse them and print the sentence,
   its parse, and its semantics; and then explain the results you get and
   exactly how you would fix the problems encountered.

      + Suzie sees Noosa.
      + Fido barks.
      + Tess barks.

#+begin_src python :results output
import nltk
from nltk import load_parser
from nltk.sem import *

nltk.download('book_grammars')
parser = load_parser('grammars/sample_grammars/sem2.fcfg', trace=0)

sentences = [
    "Suzie sees Noosa",
    "Fido barks",
    "Tess barks"
]

def print_output(sent, tree):
    print("~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~")
    print(f'Sentence: \n\t{sent}')
    print(f'Parse Result: \n{tree}')
    print(f'Semantics Result: \n\t{tree.label()["SEM"]}')
    print("~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n")


for sent in sentences:
    try:
        for tree in parser.parse(sent.split()):
            print_output(sent, tree)
    except ValueError as e:
        print("ERROR MESSAGE:")
        print(e)
#+end_src

#+RESULTS:
#+begin_example
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Sentence:
	Suzie sees Noosa
Parse Result:
(S[SEM=<see(suzie,noosa)>]
  (NP[-LOC, NUM='sg', SEM=<\P.P(suzie)>]
    (PropN[-LOC, NUM='sg', SEM=<\P.P(suzie)>] Suzie))
  (VP[NUM='sg', SEM=<\y.see(y,noosa)>]
    (TV[NUM='sg', SEM=<\X y.X(\x.see(y,x))>, TNS='pres'] sees)
    (NP[+LOC, NUM='sg', SEM=<\P.P(noosa)>]
      (PropN[+LOC, NUM='sg', SEM=<\P.P(noosa)>] Noosa))))
Semantics Result:
	see(suzie,noosa)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Sentence:
	Fido barks
Parse Result:
(S[SEM=<bark(fido)>]
  (NP[-LOC, NUM='sg', SEM=<\P.P(fido)>]
    (PropN[-LOC, NUM='sg', SEM=<\P.P(fido)>] Fido))
  (VP[NUM='sg', SEM=<\x.bark(x)>]
    (IV[NUM='sg', SEM=<\x.bark(x)>, TNS='pres'] barks)))
Semantics Result:
	bark(fido)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

ERROR MESSAGE:
Grammar does not cover some of the input words: "'Tess'".
#+end_example

/OUTPUT EXPLAINED/:
The reason for the error message is that in the grammar, "Tess" is not contained as a terminal in the grammar. It should be listed among the proper nouns, but the following is a list of those in the grammar:
#+begin_src
PropN[-LOC,NUM=sg,SEM=<\P.P(john)>] -> 'John'
PropN[-LOC,NUM=sg,SEM=<\P.P(mary)>] -> 'Mary'
PropN[-LOC,NUM=sg,SEM=<\P.P(suzie)>] -> 'Suzie'
PropN[-LOC,NUM=sg,SEM=<\P.P(fido)>] -> 'Fido'
PropN[+LOC, NUM=sg,SEM=<\P.P(noosa)>] -> 'Noosa'
#+end_src

This could be fixed by adding a line such as this:
#+begin_src
PropN[-LOC,NUM=sg,SEM=<\P.P(tess)>] -> 'Tess'
#+end_src

* Grading Scale

This homework is worth 15 points.  The grading
scale is:

| fraction correctly reviewed and answered | points awarded |
|------------------------------------------+----------------|
| \(\ge 0.95\)                             |             15 |
| 0.90 -- 0.94                             |             14 |
| 0.85 -- 0.89                             |             13 |
| 0.80 -- 0.94                             |             12 |
| 0.75 -- 0.79                             |             11 |
| 0.70 -- 0.74                             |             10 |
| 0.65 -- 0.69                             |              9 |
| 0.60 -- 0.64                             |              8 |
| 0.55 -- 0.59                             |              7 |
| 0.50 -- 0.54                             |              6 |
| 0.45 -- 0.49                             |              5 |
| 0.40 -- 0.44                             |              4 |
| 0.35 -- 0.39                             |              3 |
| 0.30 -- 0.34                             |              2 |
| 0.25 -- 0.29                             |              1 |
| \(< 0.25\)                               |              0 |




* Scoring


|     question | max pts | answer ok? |
|--------------+---------+------------|
|            1 |       1 |            |
|            2 |       1 |            |
|            3 |       1 |            |
|            4 |       1 |            |
|            5 |       1 |            |
|            6 |       2 |            |
|            7 |       1 |            |
|            8 |       1 |            |
|            9 |       1 |            |
|           10 |       1 |            |
|           11 |       2 |            |
|           12 |       2 |            |
|--------------+---------+------------|
|  total score |      15 |          0 |
|   percentage |         |          0 |
| total points |         |            |
#+TBLFM: @14$2=vsum(@I..@II)::@14$3=vsum(@I..@II)::@15$3=@-1/@-1$-1



